{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10689497,"sourceType":"datasetVersion","datasetId":6623141}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers[sentencepiece] transformers[torch] datasets evaluate sacrebleu rouge_score py7zr -q","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-07T16:59:48.175434Z","iopub.execute_input":"2025-02-07T16:59:48.175825Z","iopub.status.idle":"2025-02-07T16:59:58.416967Z","shell.execute_reply.started":"2025-02-07T16:59:48.175795Z","shell.execute_reply":"2025-02-07T16:59:58.415911Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.9/67.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.3/93.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.0/139.0 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m413.7/413.7 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM,AutoTokenizer\nfrom transformers import pipeline, set_seed\nimport matplotlib.pyplot as plt\nfrom datasets import load_dataset\nimport pandas as pd\nimport torch\nimport nltk\nfrom nltk.tokenize import sent_tokenize\nfrom tqdm import tqdm\nimport torch\n\nnltk.download(\"punkt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T17:00:03.571060Z","iopub.execute_input":"2025-02-07T17:00:03.571377Z","iopub.status.idle":"2025-02-07T17:00:27.860712Z","shell.execute_reply.started":"2025-02-07T17:00:03.571353Z","shell.execute_reply":"2025-02-07T17:00:27.859845Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":2},{"cell_type":"markdown","source":"# Teacher Model - Pegasus Large","metadata":{}},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T17:00:27.862010Z","iopub.execute_input":"2025-02-07T17:00:27.862877Z","iopub.status.idle":"2025-02-07T17:00:27.919436Z","shell.execute_reply.started":"2025-02-07T17:00:27.862848Z","shell.execute_reply":"2025-02-07T17:00:27.918540Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"\n# Define model and tokenizer. Here we use the PEGASUS model fine-tuned on XSum as an example.\nmodel_ckpt = \"google/pegasus-large\"\n\ntokenizer_lora = AutoTokenizer.from_pretrained(model_ckpt)\n\nmodel_b = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T09:39:46.221513Z","iopub.execute_input":"2025-02-07T09:39:46.221864Z","iopub.status.idle":"2025-02-07T09:40:08.709403Z","shell.execute_reply.started":"2025-02-07T09:39:46.221813Z","shell.execute_reply":"2025-02-07T09:40:08.707560Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/88.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b783b8effc5b49d3909cc279f0f51ecc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/3.09k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dcf32a06910042a1b16df0baccbc5fa1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/1.91M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67a3443de7974655b3a7ab1c7cfac78b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c04c87d8d4bf4611b92a3ecf67751682"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/2.28G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f887f2b315f04342a5278e27d7bc9036"}},"metadata":{}},{"name":"stderr","text":"Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-large and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/260 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68e17f125ed14d58813f29df931a9dd4"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"print(model_b)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T06:11:35.412367Z","iopub.execute_input":"2025-02-07T06:11:35.412703Z","iopub.status.idle":"2025-02-07T06:11:35.419604Z","shell.execute_reply.started":"2025-02-07T06:11:35.412677Z","shell.execute_reply":"2025-02-07T06:11:35.418793Z"}},"outputs":[{"name":"stdout","text":"PegasusForConditionalGeneration(\n  (model): PegasusModel(\n    (shared): Embedding(96103, 1024, padding_idx=0)\n    (encoder): PegasusEncoder(\n      (embed_tokens): Embedding(96103, 1024, padding_idx=0)\n      (embed_positions): PegasusSinusoidalPositionalEmbedding(1024, 1024)\n      (layers): ModuleList(\n        (0-15): 16 x PegasusEncoderLayer(\n          (self_attn): PegasusAttention(\n            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): ReLU()\n          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n    )\n    (decoder): PegasusDecoder(\n      (embed_tokens): Embedding(96103, 1024, padding_idx=0)\n      (embed_positions): PegasusSinusoidalPositionalEmbedding(1024, 1024)\n      (layers): ModuleList(\n        (0-15): 16 x PegasusDecoderLayer(\n          (self_attn): PegasusAttention(\n            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): PegasusAttention(\n            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (lm_head): Linear(in_features=1024, out_features=96103, bias=False)\n)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model, TaskType\n\n# Configure LoRA settings.\nlora_config = LoraConfig(\n    task_type=TaskType.SEQ_2_SEQ_LM,  # Task type for sequence-to-sequence language modeling.\n    r=8,                            # Rank of the LoRA update matrices.\n    lora_alpha=32,                  # Scaling factor.\n    lora_dropout=0.1,               # Dropout probability for the LoRA layers.\n    target_modules=[\"q_proj\",\"k_proj\", \"v_proj\",\"shared\",\"out_proj\",\"lm_head\"]       # Typically, the attention query and value projection layers.\n)\n\n# Wrap the original model with LoRA adapters.\nmodel_lora = get_peft_model(model_b, lora_config)\nmodel_lora.print_trainable_parameters()  # Optional: Print the trainable parameters for verification.\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T09:40:11.244675Z","iopub.execute_input":"2025-02-07T09:40:11.245050Z","iopub.status.idle":"2025-02-07T09:40:12.093210Z","shell.execute_reply.started":"2025-02-07T09:40:11.245008Z","shell.execute_reply":"2025-02-07T09:40:12.092123Z"}},"outputs":[{"name":"stdout","text":"trainable params: 4,699,760 || all params: 575,496,816 || trainable%: 0.8166\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/peft/tuners/tuners_utils.py:543: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['lm_head'] are part of the adapter. This can lead to complications, for example when merging the adapter or converting your model to formats other than safetensors. See for example https://github.com/huggingface/peft/issues/2018.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"x = load_dataset(\"cnn_dailymail\", \"3.0.0\")\nx","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T17:07:04.212098Z","iopub.execute_input":"2025-02-07T17:07:04.212408Z","iopub.status.idle":"2025-02-07T17:07:24.943228Z","shell.execute_reply.started":"2025-02-07T17:07:04.212386Z","shell.execute_reply":"2025-02-07T17:07:24.942423Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/15.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f03ecae82eaf42ccaaa6dddef86216b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00003.parquet:   0%|          | 0.00/257M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b2e477faa62439380e7af8738a7778a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00001-of-00003.parquet:   0%|          | 0.00/257M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14763abf410845a9ac836e39e5b31b4e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00002-of-00003.parquet:   0%|          | 0.00/259M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7ceb1eebc2d48b7b19e51204a441cab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/34.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c899b4445064d928803f9e06858e392"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/30.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5badeae96f3c44d6b91bee855555a6c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/287113 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"384c2c6688f64e009de44737c5163969"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/13368 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13beb2e05ea149a1aef64a4519e5842e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/11490 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6fc39ccec0f342e1934072934d97340e"}},"metadata":{}},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['article', 'highlights', 'id'],\n        num_rows: 287113\n    })\n    validation: Dataset({\n        features: ['article', 'highlights', 'id'],\n        num_rows: 13368\n    })\n    test: Dataset({\n        features: ['article', 'highlights', 'id'],\n        num_rows: 11490\n    })\n})"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"\ndataset = x  # Replace with your dataset name if necessary\n\n# Split the train set to retain only 1/4\ntrain_subset = dataset[\"train\"].train_test_split(test_size=0.1, seed=42)[\"test\"]\n\n# Create a new dataset dictionary\nnew_dataset = {\n    \"train\": train_subset,\n    \"validation\": dataset[\"validation\"],\n    \"test\": dataset[\"test\"]\n}\n\n# Verify the new dataset sizes\nprint(f\"Train: {len(new_dataset['train'])}\")\nprint(f\"Validation: {len(new_dataset['validation'])}\")\nprint(f\"Test: {len(new_dataset['test'])}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T17:08:31.272737Z","iopub.execute_input":"2025-02-07T17:08:31.273105Z","iopub.status.idle":"2025-02-07T17:08:31.367297Z","shell.execute_reply.started":"2025-02-07T17:08:31.273080Z","shell.execute_reply":"2025-02-07T17:08:31.366414Z"}},"outputs":[{"name":"stdout","text":"Train: 28712\nValidation: 13368\nTest: 11490\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Defining the Preprocessing Function and Tokenizing the Input Dialogues:\ndef convert_examples_to_features(example_batch):\n    input_encodings = t_tokenizer(example_batch['article'] , max_length = 1024, truncation = True )\n\n# Tokenizing the Target Summaries:\n    with t_tokenizer.as_target_tokenizer():\n        target_encodings = t_tokenizer(example_batch['highlights'], max_length = 64, truncation = True )\n\n# Returning the Tokenized Features:\n    return {\n        'input_ids' : input_encodings['input_ids'],\n        'attention_mask': input_encodings['attention_mask'],\n        'labels': target_encodings['input_ids']\n    }\n\ntrain = new_dataset['train'].map(convert_examples_to_features, batched = True)\ntest = new_dataset['test'].map(convert_examples_to_features, batched = True)\nvalid = new_dataset['validation'].map(convert_examples_to_features, batched = True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T17:09:46.725761Z","iopub.execute_input":"2025-02-07T17:09:46.726146Z","iopub.status.idle":"2025-02-07T17:11:30.267840Z","shell.execute_reply.started":"2025-02-07T17:09:46.726120Z","shell.execute_reply":"2025-02-07T17:11:30.266826Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/28712 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98eefd12b685475cba385bb140afeab8"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3953: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/11490 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7dd4c5c71a1d4c5eb2e4f18cc33bbebc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/13368 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fec2256d557144fb903d143fbde708d8"}},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"from transformers import DataCollatorForSeq2Seq\n\nseq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer_lora, model=model_lora)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T09:42:14.097603Z","iopub.execute_input":"2025-02-07T09:42:14.097969Z","iopub.status.idle":"2025-02-07T09:42:14.102210Z","shell.execute_reply.started":"2025-02-07T09:42:14.097937Z","shell.execute_reply":"2025-02-07T09:42:14.101220Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\n\ntrainer_args = TrainingArguments(\n    output_dir='teacher_model_lora',\n    num_train_epochs=1,\n    warmup_steps=500,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    weight_decay=0.01,\n    logging_steps=10,\n    evaluation_strategy='no',\n    save_steps=500,\n    gradient_accumulation_steps=16\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T17:14:02.717085Z","iopub.execute_input":"2025-02-07T17:14:02.717426Z","iopub.status.idle":"2025-02-07T17:14:02.756267Z","shell.execute_reply.started":"2025-02-07T17:14:02.717398Z","shell.execute_reply":"2025-02-07T17:14:02.755247Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"import os\nos.environ[\"WANDB_MODE\"] = \"disabled\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T09:49:41.181248Z","iopub.execute_input":"2025-02-07T09:49:41.181598Z","iopub.status.idle":"2025-02-07T09:49:41.185360Z","shell.execute_reply.started":"2025-02-07T09:49:41.181572Z","shell.execute_reply":"2025-02-07T09:49:41.184447Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"trainer = Trainer(model=model_lora, args=trainer_args,\n                  tokenizer=tokenizer_lora, data_collator=seq2seq_data_collator,\n                  train_dataset=train,\n                  eval_dataset=test)\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T09:49:44.754688Z","iopub.execute_input":"2025-02-07T09:49:44.755021Z","iopub.status.idle":"2025-02-07T11:57:03.812088Z","shell.execute_reply.started":"2025-02-07T09:49:44.754993Z","shell.execute_reply":"2025-02-07T11:57:03.811375Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-21-7de25705ea1d>:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(model=model_lora, args=trainer_args,\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1794' max='1794' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1794/1794 2:07:05, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>2.876200</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>3.061400</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>2.829800</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>2.951400</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>2.932100</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>3.222000</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>2.785600</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>3.023200</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>2.865300</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>2.967900</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>2.832500</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>2.912100</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>2.967900</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>2.846900</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>2.753200</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>2.776500</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>2.725600</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>2.895700</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>2.712600</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>2.778600</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>2.735000</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>2.559300</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>2.746100</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>2.718100</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>2.835800</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>2.501200</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>2.401500</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>2.365200</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>2.404000</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>2.510600</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>2.422000</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>2.293500</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>2.268000</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>2.341800</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>2.254800</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>2.131400</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>2.094100</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>2.110100</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>2.059600</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>2.058300</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>2.130800</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>2.109300</td>\n    </tr>\n    <tr>\n      <td>430</td>\n      <td>2.028000</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>2.007400</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>2.082800</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>2.064900</td>\n    </tr>\n    <tr>\n      <td>470</td>\n      <td>2.080700</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>1.990900</td>\n    </tr>\n    <tr>\n      <td>490</td>\n      <td>1.984200</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>1.940300</td>\n    </tr>\n    <tr>\n      <td>510</td>\n      <td>1.988500</td>\n    </tr>\n    <tr>\n      <td>520</td>\n      <td>1.964600</td>\n    </tr>\n    <tr>\n      <td>530</td>\n      <td>1.925100</td>\n    </tr>\n    <tr>\n      <td>540</td>\n      <td>1.889300</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>1.991500</td>\n    </tr>\n    <tr>\n      <td>560</td>\n      <td>1.938400</td>\n    </tr>\n    <tr>\n      <td>570</td>\n      <td>1.931900</td>\n    </tr>\n    <tr>\n      <td>580</td>\n      <td>1.910900</td>\n    </tr>\n    <tr>\n      <td>590</td>\n      <td>1.821300</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>2.041600</td>\n    </tr>\n    <tr>\n      <td>610</td>\n      <td>1.918100</td>\n    </tr>\n    <tr>\n      <td>620</td>\n      <td>2.033200</td>\n    </tr>\n    <tr>\n      <td>630</td>\n      <td>1.977800</td>\n    </tr>\n    <tr>\n      <td>640</td>\n      <td>1.905400</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>1.915600</td>\n    </tr>\n    <tr>\n      <td>660</td>\n      <td>1.974900</td>\n    </tr>\n    <tr>\n      <td>670</td>\n      <td>1.930800</td>\n    </tr>\n    <tr>\n      <td>680</td>\n      <td>1.932900</td>\n    </tr>\n    <tr>\n      <td>690</td>\n      <td>1.902300</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>1.853000</td>\n    </tr>\n    <tr>\n      <td>710</td>\n      <td>1.899400</td>\n    </tr>\n    <tr>\n      <td>720</td>\n      <td>1.855700</td>\n    </tr>\n    <tr>\n      <td>730</td>\n      <td>1.960100</td>\n    </tr>\n    <tr>\n      <td>740</td>\n      <td>1.899700</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>1.912500</td>\n    </tr>\n    <tr>\n      <td>760</td>\n      <td>1.910400</td>\n    </tr>\n    <tr>\n      <td>770</td>\n      <td>1.896400</td>\n    </tr>\n    <tr>\n      <td>780</td>\n      <td>1.872300</td>\n    </tr>\n    <tr>\n      <td>790</td>\n      <td>1.897800</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>1.904100</td>\n    </tr>\n    <tr>\n      <td>810</td>\n      <td>1.823700</td>\n    </tr>\n    <tr>\n      <td>820</td>\n      <td>1.890100</td>\n    </tr>\n    <tr>\n      <td>830</td>\n      <td>1.872600</td>\n    </tr>\n    <tr>\n      <td>840</td>\n      <td>1.857900</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>1.811300</td>\n    </tr>\n    <tr>\n      <td>860</td>\n      <td>1.881700</td>\n    </tr>\n    <tr>\n      <td>870</td>\n      <td>1.918100</td>\n    </tr>\n    <tr>\n      <td>880</td>\n      <td>1.855400</td>\n    </tr>\n    <tr>\n      <td>890</td>\n      <td>1.913000</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>1.883100</td>\n    </tr>\n    <tr>\n      <td>910</td>\n      <td>1.906700</td>\n    </tr>\n    <tr>\n      <td>920</td>\n      <td>1.802400</td>\n    </tr>\n    <tr>\n      <td>930</td>\n      <td>1.860100</td>\n    </tr>\n    <tr>\n      <td>940</td>\n      <td>1.889700</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>1.819200</td>\n    </tr>\n    <tr>\n      <td>960</td>\n      <td>1.832300</td>\n    </tr>\n    <tr>\n      <td>970</td>\n      <td>1.873100</td>\n    </tr>\n    <tr>\n      <td>980</td>\n      <td>1.862800</td>\n    </tr>\n    <tr>\n      <td>990</td>\n      <td>1.813100</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>1.798600</td>\n    </tr>\n    <tr>\n      <td>1010</td>\n      <td>1.941300</td>\n    </tr>\n    <tr>\n      <td>1020</td>\n      <td>1.925700</td>\n    </tr>\n    <tr>\n      <td>1030</td>\n      <td>1.942300</td>\n    </tr>\n    <tr>\n      <td>1040</td>\n      <td>1.921100</td>\n    </tr>\n    <tr>\n      <td>1050</td>\n      <td>1.849500</td>\n    </tr>\n    <tr>\n      <td>1060</td>\n      <td>1.866600</td>\n    </tr>\n    <tr>\n      <td>1070</td>\n      <td>1.866500</td>\n    </tr>\n    <tr>\n      <td>1080</td>\n      <td>1.846000</td>\n    </tr>\n    <tr>\n      <td>1090</td>\n      <td>1.952500</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>1.892300</td>\n    </tr>\n    <tr>\n      <td>1110</td>\n      <td>1.866300</td>\n    </tr>\n    <tr>\n      <td>1120</td>\n      <td>1.926500</td>\n    </tr>\n    <tr>\n      <td>1130</td>\n      <td>1.975800</td>\n    </tr>\n    <tr>\n      <td>1140</td>\n      <td>1.819500</td>\n    </tr>\n    <tr>\n      <td>1150</td>\n      <td>1.884800</td>\n    </tr>\n    <tr>\n      <td>1160</td>\n      <td>1.882900</td>\n    </tr>\n    <tr>\n      <td>1170</td>\n      <td>1.878700</td>\n    </tr>\n    <tr>\n      <td>1180</td>\n      <td>1.833500</td>\n    </tr>\n    <tr>\n      <td>1190</td>\n      <td>1.902100</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>1.819400</td>\n    </tr>\n    <tr>\n      <td>1210</td>\n      <td>1.865700</td>\n    </tr>\n    <tr>\n      <td>1220</td>\n      <td>1.816800</td>\n    </tr>\n    <tr>\n      <td>1230</td>\n      <td>1.892400</td>\n    </tr>\n    <tr>\n      <td>1240</td>\n      <td>1.892600</td>\n    </tr>\n    <tr>\n      <td>1250</td>\n      <td>1.799600</td>\n    </tr>\n    <tr>\n      <td>1260</td>\n      <td>1.852200</td>\n    </tr>\n    <tr>\n      <td>1270</td>\n      <td>1.871500</td>\n    </tr>\n    <tr>\n      <td>1280</td>\n      <td>1.869900</td>\n    </tr>\n    <tr>\n      <td>1290</td>\n      <td>1.822100</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>1.894700</td>\n    </tr>\n    <tr>\n      <td>1310</td>\n      <td>1.824000</td>\n    </tr>\n    <tr>\n      <td>1320</td>\n      <td>1.891600</td>\n    </tr>\n    <tr>\n      <td>1330</td>\n      <td>1.865700</td>\n    </tr>\n    <tr>\n      <td>1340</td>\n      <td>1.917600</td>\n    </tr>\n    <tr>\n      <td>1350</td>\n      <td>1.854500</td>\n    </tr>\n    <tr>\n      <td>1360</td>\n      <td>1.829900</td>\n    </tr>\n    <tr>\n      <td>1370</td>\n      <td>1.842500</td>\n    </tr>\n    <tr>\n      <td>1380</td>\n      <td>1.886700</td>\n    </tr>\n    <tr>\n      <td>1390</td>\n      <td>1.983900</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>1.860800</td>\n    </tr>\n    <tr>\n      <td>1410</td>\n      <td>1.852700</td>\n    </tr>\n    <tr>\n      <td>1420</td>\n      <td>1.818000</td>\n    </tr>\n    <tr>\n      <td>1430</td>\n      <td>1.877300</td>\n    </tr>\n    <tr>\n      <td>1440</td>\n      <td>1.887500</td>\n    </tr>\n    <tr>\n      <td>1450</td>\n      <td>1.846300</td>\n    </tr>\n    <tr>\n      <td>1460</td>\n      <td>1.889100</td>\n    </tr>\n    <tr>\n      <td>1470</td>\n      <td>1.881300</td>\n    </tr>\n    <tr>\n      <td>1480</td>\n      <td>1.897900</td>\n    </tr>\n    <tr>\n      <td>1490</td>\n      <td>1.950800</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>1.877400</td>\n    </tr>\n    <tr>\n      <td>1510</td>\n      <td>1.808700</td>\n    </tr>\n    <tr>\n      <td>1520</td>\n      <td>1.805700</td>\n    </tr>\n    <tr>\n      <td>1530</td>\n      <td>1.956300</td>\n    </tr>\n    <tr>\n      <td>1540</td>\n      <td>1.805000</td>\n    </tr>\n    <tr>\n      <td>1550</td>\n      <td>1.906500</td>\n    </tr>\n    <tr>\n      <td>1560</td>\n      <td>1.797400</td>\n    </tr>\n    <tr>\n      <td>1570</td>\n      <td>1.821000</td>\n    </tr>\n    <tr>\n      <td>1580</td>\n      <td>1.855700</td>\n    </tr>\n    <tr>\n      <td>1590</td>\n      <td>1.860600</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>1.851500</td>\n    </tr>\n    <tr>\n      <td>1610</td>\n      <td>1.928900</td>\n    </tr>\n    <tr>\n      <td>1620</td>\n      <td>1.861800</td>\n    </tr>\n    <tr>\n      <td>1630</td>\n      <td>1.897100</td>\n    </tr>\n    <tr>\n      <td>1640</td>\n      <td>1.840900</td>\n    </tr>\n    <tr>\n      <td>1650</td>\n      <td>1.848300</td>\n    </tr>\n    <tr>\n      <td>1660</td>\n      <td>1.873000</td>\n    </tr>\n    <tr>\n      <td>1670</td>\n      <td>1.785000</td>\n    </tr>\n    <tr>\n      <td>1680</td>\n      <td>1.839400</td>\n    </tr>\n    <tr>\n      <td>1690</td>\n      <td>1.902600</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>1.806900</td>\n    </tr>\n    <tr>\n      <td>1710</td>\n      <td>1.938200</td>\n    </tr>\n    <tr>\n      <td>1720</td>\n      <td>1.907000</td>\n    </tr>\n    <tr>\n      <td>1730</td>\n      <td>1.832500</td>\n    </tr>\n    <tr>\n      <td>1740</td>\n      <td>1.863800</td>\n    </tr>\n    <tr>\n      <td>1750</td>\n      <td>1.860500</td>\n    </tr>\n    <tr>\n      <td>1760</td>\n      <td>1.826000</td>\n    </tr>\n    <tr>\n      <td>1770</td>\n      <td>1.808700</td>\n    </tr>\n    <tr>\n      <td>1780</td>\n      <td>1.825500</td>\n    </tr>\n    <tr>\n      <td>1790</td>\n      <td>1.855800</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n","output_type":"stream"},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1794, training_loss=2.058553509356053, metrics={'train_runtime': 7636.1577, 'train_samples_per_second': 3.76, 'train_steps_per_second': 0.235, 'total_flos': 6.152736813076339e+16, 'train_loss': 2.058553509356053, 'epoch': 0.9997213708553915})"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"model_lora.save_pretrained(\"./teacher_modelN\")\ntokenizer_lora.save_pretrained(\"./teacher_modelN\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T11:57:56.904348Z","iopub.execute_input":"2025-02-07T11:57:56.905094Z","iopub.status.idle":"2025-02-07T11:57:59.262446Z","shell.execute_reply.started":"2025-02-07T11:57:56.905053Z","shell.execute_reply":"2025-02-07T11:57:59.261607Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n","output_type":"stream"},{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"('./teacher_modelN/tokenizer_config.json',\n './teacher_modelN/special_tokens_map.json',\n './teacher_modelN/spiece.model',\n './teacher_modelN/added_tokens.json',\n './teacher_modelN/tokenizer.json')"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T09:43:22.899601Z","iopub.execute_input":"2025-02-07T09:43:22.899922Z","iopub.status.idle":"2025-02-07T09:44:04.405213Z","shell.execute_reply.started":"2025-02-07T09:43:22.899899Z","shell.execute_reply":"2025-02-07T09:44:04.404323Z"}},"outputs":[{"name":"stdout","text":"Folder successfully zipped as teacher_model.zip\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"!zip -r file2.zip /kaggle/working/teacher_modelN","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T11:58:20.073455Z","iopub.execute_input":"2025-02-07T11:58:20.073937Z","iopub.status.idle":"2025-02-07T11:59:06.955543Z","shell.execute_reply.started":"2025-02-07T11:58:20.073884Z","shell.execute_reply":"2025-02-07T11:59:06.954505Z"}},"outputs":[{"name":"stdout","text":"  adding: kaggle/working/teacher_modelN/ (stored 0%)\n  adding: kaggle/working/teacher_modelN/tokenizer.json (deflated 78%)\n  adding: kaggle/working/teacher_modelN/adapter_config.json (deflated 55%)\n  adding: kaggle/working/teacher_modelN/tokenizer_config.json (deflated 94%)\n  adding: kaggle/working/teacher_modelN/adapter_model.safetensors (deflated 7%)\n  adding: kaggle/working/teacher_modelN/special_tokens_map.json (deflated 82%)\n  adding: kaggle/working/teacher_modelN/spiece.model (deflated 50%)\n  adding: kaggle/working/teacher_modelN/README.md (deflated 66%)\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'file2.zip')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T11:59:16.781780Z","iopub.execute_input":"2025-02-07T11:59:16.782157Z","iopub.status.idle":"2025-02-07T11:59:16.788928Z","shell.execute_reply.started":"2025-02-07T11:59:16.782121Z","shell.execute_reply":"2025-02-07T11:59:16.788103Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/file2.zip","text/html":"<a href='file2.zip' target='_blank'>file2.zip</a><br>"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"import shutil\n\n# Define the folder to zip and the output file name\nfolder_path = '/kaggle/working/teacher_model_lora'\noutput_filename = 'teacher_model_checkpoints'\n\n# Create a zip archive\nshutil.make_archive(output_filename, 'zip', folder_path)\n\nprint(f\"Folder successfully zipped as {output_filename}.zip\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"inference","metadata":{}},{"cell_type":"code","source":"text_to_summarize = \"\"\"\nWe propose a memory-efficient finetuning algorithm for large language models (LLMs) that supports finetuning LLMs with 65B parameters in 2/3/4-bit precision on as little as one 24GB GPU. Our method, modular low-rank adaptation (ModuLoRA), integrates any user-specified weight quantizer with finetuning via low-rank adapters (LoRAs). Our approach relies on a simple quantization-agnostic backward pass that adaptively materializes low-precision LLM weights from a custom black-box quantization module. This approach enables finetuning 2-bit and 3-bit LLMs for the first time---leveraging state-of-the-art 2-bit QuIP# quantization and 3-bit OPTQ quantization---outperforming finetuning that relies on less sophisticated 4-bit and 8-bit methods. In our experiments, ModuLoRA attains competitive performance on text classification, natural language infernece, and instruction following tasks using significantly less memory than existing approaches, and we also surpass the state-of-the-art ROUGE score on a popular summarization task. We release ModuLoRA together with a series of low-precision models as part of LLMTOOLS, a user-friendly library for quantizing, running, and finetuning LLMs on consumer GPUs.\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T11:59:44.765239Z","iopub.execute_input":"2025-02-07T11:59:44.765556Z","iopub.status.idle":"2025-02-07T11:59:44.769687Z","shell.execute_reply.started":"2025-02-07T11:59:44.765533Z","shell.execute_reply":"2025-02-07T11:59:44.768870Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# Tokenize input and move tensors to the same device as the model\ninputs = tokenizer_lora.encode(\"summarize: \" + text_to_summarize, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n\n# Generate summary (pass inputs as a keyword argument)\nsummary_ids = model_lora.generate(input_ids=inputs, max_length=150, num_beams=4, early_stopping=True)\n\n# Decode and print the summary\nsummary_text = tokenizer_lora.decode(summary_ids[0], skip_special_tokens=True)\nprint(\"Summarized Text:\", summary_text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T11:59:47.634000Z","iopub.execute_input":"2025-02-07T11:59:47.634379Z","iopub.status.idle":"2025-02-07T11:59:50.281610Z","shell.execute_reply.started":"2025-02-07T11:59:47.634351Z","shell.execute_reply":"2025-02-07T11:59:50.280789Z"}},"outputs":[{"name":"stdout","text":"Summarized Text: This approach enables finetuning 2-bit and 3-bit LLMs for the first time---leveraging state-of-the-art 2-bit QuIP# quantization and 3-bit OPTQ quantization---outperforming finetuning that relies on less sophisticated 4- and 8-bit methods.\n","output_type":"stream"}],"execution_count":26},{"cell_type":"markdown","source":"Performance evaluation","metadata":{}},{"cell_type":"code","source":"def generate_batch_sized_chunks(list_of_elements, batch_size):\n    \"\"\"split the dataset into smaller batches that we can process simultaneously\n    Yield successive batch-sized chunks from list_of_elements.\"\"\"\n    for i in range(0, len(list_of_elements), batch_size):\n        yield list_of_elements[i : i + batch_size]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T20:25:51.732164Z","iopub.execute_input":"2025-02-07T20:25:51.732521Z","iopub.status.idle":"2025-02-07T20:25:51.737328Z","shell.execute_reply.started":"2025-02-07T20:25:51.732467Z","shell.execute_reply":"2025-02-07T20:25:51.736219Z"}},"outputs":[],"execution_count":69},{"cell_type":"code","source":"device","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T20:37:21.791148Z","iopub.status.idle":"2025-02-07T20:37:21.791564Z","shell.execute_reply":"2025-02-07T20:37:21.791411Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import evaluate\n\ndef calculate_metric_on_test_ds(dataset, metric, model, tokenizer,\n                               batch_size=16, device=None,\n                               column_text=\"article\",\n                               column_summary=\"highlights\"):\n    # Default to GPU if available, otherwise use CPU\n    if device is None:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # Move the model to the correct device (GPU/CPU)\n    model.to(device)\n\n    # Splitting the dataset into batches\n    article_batches = list(generate_batch_sized_chunks(dataset[column_text], batch_size))\n    target_batches = list(generate_batch_sized_chunks(dataset[column_summary], batch_size))\n\n    # Processing each batch\n    for article_batch, target_batch in tqdm(\n        zip(article_batches, target_batches), total=len(article_batches)):\n\n        # Tokenizing the articles:\n        inputs = tokenizer(article_batch, max_length=1024, truncation=True,\n                           padding=\"max_length\", return_tensors=\"pt\")\n\n        # Move inputs to the same device as the model\n        inputs = {key: value.to(device) for key, value in inputs.items()}\n\n        # Generating summaries:\n        summaries = model.generate(input_ids=inputs[\"input_ids\"],\n                                   attention_mask=inputs[\"attention_mask\"],\n                                   length_penalty=0.8, num_beams=8, max_length=128)\n\n        # Decode the generated summaries\n        decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n                             for s in summaries]\n\n        # Replace unwanted tokens (like empty spaces)\n        decoded_summaries = [d.replace(\"\", \" \") for d in decoded_summaries]\n\n        # Updating the metric\n        metric.add_batch(predictions=decoded_summaries, references=target_batch)\n\n    # Compute and return the ROUGE scores.\n    score = metric.compute()\n    return score\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T20:28:05.730875Z","iopub.execute_input":"2025-02-07T20:28:05.731248Z","iopub.status.idle":"2025-02-07T20:28:05.780900Z","shell.execute_reply.started":"2025-02-07T20:28:05.731226Z","shell.execute_reply":"2025-02-07T20:28:05.780104Z"}},"outputs":[],"execution_count":71},{"cell_type":"code","source":"rouge_metric = evaluate.load(\"rouge\")\nscore = calculate_metric_on_test_ds(\n    valid, rouge_metric, trainer.model, tokenizer_lora, batch_size = 2, column_text = 'dialogue', column_summary= 'summary'\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T20:28:17.700427Z","iopub.execute_input":"2025-02-07T20:28:17.700805Z","iopub.status.idle":"2025-02-07T20:28:18.190685Z","shell.execute_reply.started":"2025-02-07T20:28:17.700776Z","shell.execute_reply":"2025-02-07T20:28:18.189579Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"185b00af90ac489b913800d320d28029"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-72-94ad08063b34>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrouge_metric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rouge\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m score = calculate_metric_on_test_ds(\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mvalid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrouge_metric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer_lora\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'dialogue'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn_summary\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'summary'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m )\n","\u001b[0;31mNameError\u001b[0m: name 'tokenizer_lora' is not defined"],"ename":"NameError","evalue":"name 'tokenizer_lora' is not defined","output_type":"error"}],"execution_count":72},{"cell_type":"markdown","source":"Load the model","metadata":{}},{"cell_type":"code","source":"from transformers import PegasusForConditionalGeneration, PegasusTokenizer\nfrom peft import PeftModel\n\n# Define paths\nbase_model_name = \"google/pegasus-large\"  # Replace with your base model\nlora_checkpoint_path = \"/kaggle/input/peg-large/kaggle/working/teacher_modelN\"  # Replace with your LoRA checkpoint path\n\n# Load the base Pegasus model and tokenizer\nt_tokenizer = AutoTokenizer.from_pretrained(base_model_name)\nbase_model = AutoModelForSeq2SeqLM.from_pretrained(base_model_name)\n\n# Load the LoRA weights into the base model\nteacher_model = PeftModel.from_pretrained(base_model, lora_checkpoint_path)\n\nprint(\"LoRA model loaded successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T17:01:48.810096Z","iopub.execute_input":"2025-02-07T17:01:48.810448Z","iopub.status.idle":"2025-02-07T17:02:39.566222Z","shell.execute_reply.started":"2025-02-07T17:01:48.810424Z","shell.execute_reply":"2025-02-07T17:02:39.564422Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/88.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d227a55af269460db7f7b256dbf48421"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/3.09k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a950e926b4e14896a215278f4ffe9ed6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/1.91M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1759ad000a90447e91c16e49c2e4dbf1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26766086cac849218f29c0deed171dca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/2.28G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"281561901f0c46f3a4f00ae01943baae"}},"metadata":{}},{"name":"stderr","text":"Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-large and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/260 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64fc49a74cf24e73a21645b3930a3990"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/peft/tuners/tuners_utils.py:543: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['lm_head'] are part of the adapter. This can lead to complications, for example when merging the adapter or converting your model to formats other than safetensors. See for example https://github.com/huggingface/peft/issues/2018.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"LoRA model loaded successfully!\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"print(teacher_model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T17:02:55.364325Z","iopub.execute_input":"2025-02-07T17:02:55.364694Z","iopub.status.idle":"2025-02-07T17:02:55.380962Z","shell.execute_reply.started":"2025-02-07T17:02:55.364668Z","shell.execute_reply":"2025-02-07T17:02:55.379995Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"PeftModelForSeq2SeqLM(\n  (base_model): LoraModel(\n    (model): PegasusForConditionalGeneration(\n      (model): PegasusModel(\n        (shared): lora.Embedding(\n          (base_layer): Embedding(96103, 1024, padding_idx=0)\n          (lora_dropout): ModuleDict(\n            (default): Dropout(p=0.1, inplace=False)\n          )\n          (lora_A): ModuleDict()\n          (lora_B): ModuleDict()\n          (lora_embedding_A): ParameterDict(  (default): Parameter containing: [torch.FloatTensor of size 8x96103])\n          (lora_embedding_B): ParameterDict(  (default): Parameter containing: [torch.FloatTensor of size 1024x8])\n          (lora_magnitude_vector): ModuleDict()\n        )\n        (encoder): PegasusEncoder(\n          (embed_tokens): Embedding(96103, 1024, padding_idx=0)\n          (embed_positions): PegasusSinusoidalPositionalEmbedding(1024, 1024)\n          (layers): ModuleList(\n            (0-15): 16 x PegasusEncoderLayer(\n              (self_attn): PegasusAttention(\n                (k_proj): lora.Linear(\n                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.1, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1024, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=1024, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (v_proj): lora.Linear(\n                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.1, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1024, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=1024, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (q_proj): lora.Linear(\n                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.1, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1024, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=1024, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (out_proj): lora.Linear(\n                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.1, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1024, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=1024, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n              )\n              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (activation_fn): ReLU()\n              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            )\n          )\n          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        )\n        (decoder): PegasusDecoder(\n          (embed_tokens): Embedding(96103, 1024, padding_idx=0)\n          (embed_positions): PegasusSinusoidalPositionalEmbedding(1024, 1024)\n          (layers): ModuleList(\n            (0-15): 16 x PegasusDecoderLayer(\n              (self_attn): PegasusAttention(\n                (k_proj): lora.Linear(\n                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.1, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1024, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=1024, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (v_proj): lora.Linear(\n                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.1, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1024, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=1024, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (q_proj): lora.Linear(\n                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.1, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1024, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=1024, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (out_proj): lora.Linear(\n                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.1, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1024, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=1024, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n              )\n              (activation_fn): ReLU()\n              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (encoder_attn): PegasusAttention(\n                (k_proj): lora.Linear(\n                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.1, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1024, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=1024, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (v_proj): lora.Linear(\n                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.1, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1024, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=1024, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (q_proj): lora.Linear(\n                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.1, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1024, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=1024, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (out_proj): lora.Linear(\n                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.1, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1024, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=1024, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n              )\n              (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            )\n          )\n          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (lm_head): lora.Linear(\n        (base_layer): Linear(in_features=1024, out_features=96103, bias=False)\n        (lora_dropout): ModuleDict(\n          (default): Dropout(p=0.1, inplace=False)\n        )\n        (lora_A): ModuleDict(\n          (default): Linear(in_features=1024, out_features=8, bias=False)\n        )\n        (lora_B): ModuleDict(\n          (default): Linear(in_features=8, out_features=96103, bias=False)\n        )\n        (lora_embedding_A): ParameterDict()\n        (lora_embedding_B): ParameterDict()\n        (lora_magnitude_vector): ModuleDict()\n      )\n    )\n  )\n)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# Student Model","metadata":{}},{"cell_type":"code","source":"# Define model and tokenizer. Here we use the PEGASUS model fine-tuned on XSum as an example.\nmodel_ckpt = \"google/pegasus-x-base\"\n\ns_tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T17:03:33.904292Z","iopub.execute_input":"2025-02-07T17:03:33.904696Z","iopub.status.idle":"2025-02-07T17:03:51.131504Z","shell.execute_reply.started":"2025-02-07T17:03:33.904668Z","shell.execute_reply":"2025-02-07T17:03:51.130016Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.02k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"427e9f9db8184af5bda9ad51f2ac1cd8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/1.91M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a233ad423ab74b3481b5a4d6f8e9a333"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/6.60M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e60fef8bcdd49d19f7fa99c613de0ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/1.77k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a80d74a4a820473c838a4e04f4882325"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.49k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dafffa3327124b6e8aae8428b1f0c227"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.09G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0c8afbf6fa84f73952f331ea9fc151c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/262 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d4376a91323414a96ea0337581f458f"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model, TaskType\n\n# Configure LoRA settings.\nlora_config = LoraConfig(\n    task_type=TaskType.SEQ_2_SEQ_LM,  # Task type for sequence-to-sequence language modeling.\n    r=8,                            # Rank of the LoRA update matrices.\n    lora_alpha=32,                  # Scaling factor.\n    lora_dropout=0.1,               # Dropout probability for the LoRA layers.\n    target_modules=[\"q_proj\",\"k_proj\", \"v_proj\",\"shared\",\"out_proj\",\"lm_head\"]       # Typically, the attention query and value projection layers.\n)\n\n# Wrap the original model with LoRA adapters.\ns_model = get_peft_model(model, lora_config)\ns_model.print_trainable_parameters()  # Optional: Print the trainable parameters for verification.\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T17:04:03.832905Z","iopub.execute_input":"2025-02-07T17:04:03.833402Z","iopub.status.idle":"2025-02-07T17:04:04.073023Z","shell.execute_reply.started":"2025-02-07T17:04:03.833366Z","shell.execute_reply":"2025-02-07T17:04:04.071892Z"}},"outputs":[{"name":"stdout","text":"trainable params: 3,319,408 || all params: 275,611,504 || trainable%: 1.2044\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T12:03:30.028158Z","iopub.execute_input":"2025-02-07T12:03:30.028478Z","iopub.status.idle":"2025-02-07T12:03:30.033970Z","shell.execute_reply.started":"2025-02-07T12:03:30.028453Z","shell.execute_reply":"2025-02-07T12:03:30.032996Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":31},{"cell_type":"markdown","source":"Teacher training - cross entropy and distillation loss","metadata":{}},{"cell_type":"code","source":"import torch\n\ndef shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start_token_id: int) -> torch.Tensor:\n    \"\"\"\n    Shift input ids one token to the right, and wrap the last token in the sequence as the first token in the output.\n    This is required for decoder input preparation during training.\n    \"\"\"\n    shifted_input_ids = input_ids.clone()\n    shifted_input_ids[..., 1:] = input_ids[..., :-1].clone()\n    shifted_input_ids[..., 0] = decoder_start_token_id\n\n    if pad_token_id is not None:\n        # Replace any -100s in the labels with the pad token ID to prevent loss errors\n        shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)\n\n    return shifted_input_ids\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T17:26:39.045646Z","iopub.execute_input":"2025-02-07T17:26:39.045992Z","iopub.status.idle":"2025-02-07T17:26:39.051587Z","shell.execute_reply.started":"2025-02-07T17:26:39.045969Z","shell.execute_reply":"2025-02-07T17:26:39.050589Z"}},"outputs":[],"execution_count":52},{"cell_type":"code","source":"trainer_args = TrainingArguments(\n    output_dir='baap',\n    num_train_epochs=1,\n    warmup_steps=500,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    weight_decay=0.01,\n    logging_steps=100,\n    evaluation_strategy='no',\n    save_steps=500,\n    gradient_accumulation_steps=16\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T17:24:33.685843Z","iopub.execute_input":"2025-02-07T17:24:33.686166Z","iopub.status.idle":"2025-02-07T17:24:33.719660Z","shell.execute_reply.started":"2025-02-07T17:24:33.686141Z","shell.execute_reply":"2025-02-07T17:24:33.718593Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"import torch\nfrom torch.nn import KLDivLoss, CrossEntropyLoss\nfrom transformers import Trainer\n\nclass DistillationTrainer(Trainer):\n    def __init__(self, teacher_model, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.teacher_model = teacher_model\n\n    def compute_loss(self, model, inputs, return_outputs=False,**kwargs):\n        device = model.device\n\n        # Move inputs to the same device as the model\n        inputs = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}\n\n        labels = inputs[\"labels\"]\n        decoder_input_ids = shift_tokens_right(labels, model.config.pad_token_id, model.config.decoder_start_token_id)\n        inputs[\"decoder_input_ids\"] = decoder_input_ids\n\n        # Forward pass on the student\n        student_outputs = model(**inputs)\n\n        # Forward pass on the teacher\n        self.teacher_model.to(device)  # Ensure the teacher model is on the same device\n        with torch.no_grad():\n            teacher_outputs = self.teacher_model(**inputs)\n\n        # Extract logits\n        student_logits = student_outputs.logits\n        teacher_logits = teacher_outputs.logits\n\n        # Loss functions\n        kl_loss_fn = KLDivLoss(reduction=\"batchmean\")\n        ce_loss_fn = CrossEntropyLoss()\n\n        # KL Loss between teacher and student distributions\n        kl_loss = kl_loss_fn(\n            torch.log_softmax(student_logits / 2.0, dim=-1),\n            torch.softmax(teacher_logits / 2.0, dim=-1)\n        )\n\n        # Hard label loss\n        ce_loss = ce_loss_fn(student_logits.view(-1, student_logits.size(-1)), labels.view(-1))\n\n        # Combine losses\n        loss = 0.5 * kl_loss + 0.5 * ce_loss\n\n        return (loss, student_outputs) if return_outputs else loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T17:30:45.772713Z","iopub.execute_input":"2025-02-07T17:30:45.773044Z","iopub.status.idle":"2025-02-07T17:30:45.783674Z","shell.execute_reply.started":"2025-02-07T17:30:45.773020Z","shell.execute_reply":"2025-02-07T17:30:45.782532Z"}},"outputs":[],"execution_count":59},{"cell_type":"code","source":"import os\nos.environ[\"WANDB_MODE\"] = \"disabled\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T17:24:36.251881Z","iopub.execute_input":"2025-02-07T17:24:36.252222Z","iopub.status.idle":"2025-02-07T17:24:36.256326Z","shell.execute_reply.started":"2025-02-07T17:24:36.252197Z","shell.execute_reply":"2025-02-07T17:24:36.255377Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"# Initialize the distillation trainer\ntrainer = DistillationTrainer(\n    teacher_model=teacher_model,  # Pass the teacher model explicitly\n    model=s_model,\n    args=trainer_args,\n    train_dataset=train,\n    eval_dataset=test,\n    tokenizer=t_tokenizer\n)\n\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T17:32:04.489515Z","iopub.execute_input":"2025-02-07T17:32:04.489864Z","iopub.status.idle":"2025-02-07T20:21:45.456463Z","shell.execute_reply.started":"2025-02-07T17:32:04.489837Z","shell.execute_reply":"2025-02-07T20:21:45.455657Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-59-15bb725e6ea5>:7: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `DistillationTrainer.__init__`. Use `processing_class` instead.\n  super().__init__(*args, **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1794' max='1794' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1794/1794 2:49:34, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>52.952700</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>43.068300</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>29.518400</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>19.428100</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>14.667800</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>12.804900</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>12.031300</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>11.522000</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>11.252500</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>11.069400</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>11.107700</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>10.903500</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>10.824600</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>10.811000</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>10.677200</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>10.630200</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>10.642500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n","output_type":"stream"},{"execution_count":61,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1794, training_loss=16.9464596479367, metrics={'train_runtime': 10180.3273, 'train_samples_per_second': 2.82, 'train_steps_per_second': 0.176, 'total_flos': 2.6127792598317696e+16, 'train_loss': 16.9464596479367, 'epoch': 0.9997213708553915})"},"metadata":{}}],"execution_count":61},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"s_model.save_pretrained(\"distilled_pegasus_base\")\ns_tokenizer.save_pretrained(\"distilled_pegasus_base\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T20:22:20.917730Z","iopub.execute_input":"2025-02-07T20:22:20.918042Z","iopub.status.idle":"2025-02-07T20:22:22.762996Z","shell.execute_reply.started":"2025-02-07T20:22:20.918018Z","shell.execute_reply":"2025-02-07T20:22:22.762114Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n","output_type":"stream"},{"execution_count":62,"output_type":"execute_result","data":{"text/plain":"('distilled_pegasus_base/tokenizer_config.json',\n 'distilled_pegasus_base/special_tokens_map.json',\n 'distilled_pegasus_base/spiece.model',\n 'distilled_pegasus_base/added_tokens.json',\n 'distilled_pegasus_base/tokenizer.json')"},"metadata":{}}],"execution_count":62},{"cell_type":"code","source":"!zip -r baap.zip /kaggle/working/baap","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T20:58:40.483206Z","iopub.execute_input":"2025-02-07T20:58:40.483674Z","iopub.status.idle":"2025-02-07T21:01:04.112667Z","shell.execute_reply.started":"2025-02-07T20:58:40.483639Z","shell.execute_reply":"2025-02-07T21:01:04.111627Z"}},"outputs":[{"name":"stdout","text":"  adding: kaggle/working/baap/ (stored 0%)\n  adding: kaggle/working/baap/checkpoint-1794/ (stored 0%)\n  adding: kaggle/working/baap/checkpoint-1794/adapter_config.json (deflated 55%)\n  adding: kaggle/working/baap/checkpoint-1794/tokenizer_config.json (deflated 94%)\n  adding: kaggle/working/baap/checkpoint-1794/training_args.bin (deflated 51%)\n  adding: kaggle/working/baap/checkpoint-1794/scheduler.pt (deflated 56%)\n  adding: kaggle/working/baap/checkpoint-1794/optimizer.pt (deflated 8%)\n  adding: kaggle/working/baap/checkpoint-1794/trainer_state.json (deflated 69%)\n  adding: kaggle/working/baap/checkpoint-1794/rng_state.pth (deflated 25%)\n  adding: kaggle/working/baap/checkpoint-1794/special_tokens_map.json (deflated 82%)\n  adding: kaggle/working/baap/checkpoint-1794/spiece.model (deflated 50%)\n  adding: kaggle/working/baap/checkpoint-1794/adapter_model.safetensors (deflated 7%)\n  adding: kaggle/working/baap/checkpoint-1794/tokenizer.json (deflated 78%)\n  adding: kaggle/working/baap/checkpoint-1794/README.md (deflated 66%)\n  adding: kaggle/working/baap/checkpoint-500/ (stored 0%)\n  adding: kaggle/working/baap/checkpoint-500/adapter_config.json (deflated 55%)\n  adding: kaggle/working/baap/checkpoint-500/tokenizer_config.json (deflated 94%)\n  adding: kaggle/working/baap/checkpoint-500/training_args.bin (deflated 51%)\n  adding: kaggle/working/baap/checkpoint-500/scheduler.pt (deflated 57%)\n  adding: kaggle/working/baap/checkpoint-500/optimizer.pt (deflated 7%)\n  adding: kaggle/working/baap/checkpoint-500/trainer_state.json (deflated 62%)\n  adding: kaggle/working/baap/checkpoint-500/rng_state.pth (deflated 25%)\n  adding: kaggle/working/baap/checkpoint-500/special_tokens_map.json (deflated 82%)\n  adding: kaggle/working/baap/checkpoint-500/spiece.model (deflated 50%)\n  adding: kaggle/working/baap/checkpoint-500/adapter_model.safetensors (deflated 7%)\n  adding: kaggle/working/baap/checkpoint-500/tokenizer.json (deflated 78%)\n  adding: kaggle/working/baap/checkpoint-500/README.md (deflated 66%)\n  adding: kaggle/working/baap/checkpoint-1000/ (stored 0%)\n  adding: kaggle/working/baap/checkpoint-1000/adapter_config.json (deflated 55%)\n  adding: kaggle/working/baap/checkpoint-1000/tokenizer_config.json (deflated 94%)\n  adding: kaggle/working/baap/checkpoint-1000/training_args.bin (deflated 51%)\n  adding: kaggle/working/baap/checkpoint-1000/scheduler.pt (deflated 55%)\n  adding: kaggle/working/baap/checkpoint-1000/optimizer.pt (deflated 8%)\n  adding: kaggle/working/baap/checkpoint-1000/trainer_state.json (deflated 67%)\n  adding: kaggle/working/baap/checkpoint-1000/rng_state.pth (deflated 25%)\n  adding: kaggle/working/baap/checkpoint-1000/special_tokens_map.json (deflated 82%)\n  adding: kaggle/working/baap/checkpoint-1000/spiece.model (deflated 50%)\n  adding: kaggle/working/baap/checkpoint-1000/adapter_model.safetensors (deflated 7%)\n  adding: kaggle/working/baap/checkpoint-1000/tokenizer.json (deflated 78%)\n  adding: kaggle/working/baap/checkpoint-1000/README.md (deflated 66%)\n  adding: kaggle/working/baap/runs/ (stored 0%)\n  adding: kaggle/working/baap/runs/Feb07_17-24-33_f837b28388d3/ (stored 0%)\n  adding: kaggle/working/baap/runs/Feb07_17-24-33_f837b28388d3/events.out.tfevents.1738949525.f837b28388d3.31.7 (deflated 62%)\n  adding: kaggle/working/baap/runs/Feb07_17-24-33_f837b28388d3/events.out.tfevents.1738949222.f837b28388d3.31.4 (deflated 63%)\n  adding: kaggle/working/baap/runs/Feb07_17-24-33_f837b28388d3/events.out.tfevents.1738949414.f837b28388d3.31.5 (deflated 63%)\n  adding: kaggle/working/baap/runs/Feb07_17-24-33_f837b28388d3/events.out.tfevents.1738949079.f837b28388d3.31.3 (deflated 63%)\n  adding: kaggle/working/baap/runs/Feb07_17-24-33_f837b28388d3/events.out.tfevents.1738949450.f837b28388d3.31.6 (deflated 63%)\n  adding: kaggle/working/baap/runs/Feb07_17-22-49_f837b28388d3/ (stored 0%)\n  adding: kaggle/working/baap/runs/Feb07_17-22-49_f837b28388d3/events.out.tfevents.1738949018.f837b28388d3.31.2 (deflated 63%)\n  adding: kaggle/working/baap/checkpoint-1500/ (stored 0%)\n  adding: kaggle/working/baap/checkpoint-1500/adapter_config.json (deflated 55%)\n  adding: kaggle/working/baap/checkpoint-1500/tokenizer_config.json (deflated 94%)\n  adding: kaggle/working/baap/checkpoint-1500/training_args.bin (deflated 51%)\n  adding: kaggle/working/baap/checkpoint-1500/scheduler.pt (deflated 55%)\n  adding: kaggle/working/baap/checkpoint-1500/optimizer.pt (deflated 8%)\n  adding: kaggle/working/baap/checkpoint-1500/trainer_state.json (deflated 69%)\n  adding: kaggle/working/baap/checkpoint-1500/rng_state.pth (deflated 25%)\n  adding: kaggle/working/baap/checkpoint-1500/special_tokens_map.json (deflated 82%)\n  adding: kaggle/working/baap/checkpoint-1500/spiece.model (deflated 50%)\n  adding: kaggle/working/baap/checkpoint-1500/adapter_model.safetensors (deflated 7%)\n  adding: kaggle/working/baap/checkpoint-1500/tokenizer.json (deflated 78%)\n  adding: kaggle/working/baap/checkpoint-1500/README.md (deflated 66%)\n","output_type":"stream"}],"execution_count":100},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'baap.zip')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T21:01:26.413321Z","iopub.execute_input":"2025-02-07T21:01:26.413730Z","iopub.status.idle":"2025-02-07T21:01:26.420512Z","shell.execute_reply.started":"2025-02-07T21:01:26.413699Z","shell.execute_reply":"2025-02-07T21:01:26.419417Z"}},"outputs":[{"execution_count":101,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/baap.zip","text/html":"<a href='baap.zip' target='_blank'>baap.zip</a><br>"},"metadata":{}}],"execution_count":101},{"cell_type":"code","source":"!zip -r distilled_pegasus.zip /kaggle/working/distilled_pegasus_base","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T20:22:58.210399Z","iopub.execute_input":"2025-02-07T20:22:58.210776Z","iopub.status.idle":"2025-02-07T20:23:33.159714Z","shell.execute_reply.started":"2025-02-07T20:22:58.210736Z","shell.execute_reply":"2025-02-07T20:23:33.158422Z"}},"outputs":[{"name":"stdout","text":"  adding: kaggle/working/distilled_pegasus_base/ (stored 0%)\n  adding: kaggle/working/distilled_pegasus_base/adapter_config.json (deflated 55%)\n  adding: kaggle/working/distilled_pegasus_base/tokenizer_config.json (deflated 94%)\n  adding: kaggle/working/distilled_pegasus_base/special_tokens_map.json (deflated 83%)\n  adding: kaggle/working/distilled_pegasus_base/spiece.model (deflated 50%)\n  adding: kaggle/working/distilled_pegasus_base/adapter_model.safetensors (deflated 7%)\n  adding: kaggle/working/distilled_pegasus_base/tokenizer.json (deflated 78%)\n  adding: kaggle/working/distilled_pegasus_base/README.md (deflated 66%)\n","output_type":"stream"}],"execution_count":63},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'distilled_pegasus.zip')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T20:23:45.098006Z","iopub.execute_input":"2025-02-07T20:23:45.098368Z","iopub.status.idle":"2025-02-07T20:23:45.104689Z","shell.execute_reply.started":"2025-02-07T20:23:45.098345Z","shell.execute_reply":"2025-02-07T20:23:45.103832Z"}},"outputs":[{"execution_count":64,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/distilled_pegasus.zip","text/html":"<a href='distilled_pegasus.zip' target='_blank'>distilled_pegasus.zip</a><br>"},"metadata":{}}],"execution_count":64},{"cell_type":"markdown","source":"inference","metadata":{}},{"cell_type":"code","source":"text_to_summarize = \"\"\"\nWe propose a memory-efficient finetuning algorithm for large language models (LLMs) that supports finetuning LLMs with 65B parameters in 2/3/4-bit precision on as little as one 24GB GPU. Our method, modular low-rank adaptation (ModuLoRA), integrates any user-specified weight quantizer with finetuning via low-rank adapters (LoRAs). Our approach relies on a simple quantization-agnostic backward pass that adaptively materializes low-precision LLM weights from a custom black-box quantization module. This approach enables finetuning 2-bit and 3-bit LLMs for the first time---leveraging state-of-the-art 2-bit QuIP# quantization and 3-bit OPTQ quantization---outperforming finetuning that relies on less sophisticated 4-bit and 8-bit methods. In our experiments, ModuLoRA attains competitive performance on text classification, natural language infernece, and instruction following tasks using significantly less memory than existing approaches, and we also surpass the state-of-the-art ROUGE score on a popular summarization task. We release ModuLoRA together with a series of low-precision models as part of LLMTOOLS, a user-friendly library for quantizing, running, and finetuning LLMs on consumer GPUs.\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T20:24:48.850368Z","iopub.execute_input":"2025-02-07T20:24:48.850792Z","iopub.status.idle":"2025-02-07T20:24:48.855000Z","shell.execute_reply.started":"2025-02-07T20:24:48.850761Z","shell.execute_reply":"2025-02-07T20:24:48.853970Z"}},"outputs":[],"execution_count":65},{"cell_type":"code","source":"# Tokenize input and move tensors to the same device as the model\ninputs = s_tokenizer.encode(\"summarize: \" + valid[2]['article'] , return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n\n# Generate summary (pass inputs as a keyword argument)\nsummary_ids = s_model.generate(input_ids=inputs, max_length=150, num_beams=4, early_stopping=True)\n\n# Decode and print the summary\nsummary_text = s_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\nprint(\"Summarized Text:\", summary_text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T20:37:48.795233Z","iopub.execute_input":"2025-02-07T20:37:48.795648Z","iopub.status.idle":"2025-02-07T20:37:49.632866Z","shell.execute_reply.started":"2025-02-07T20:37:48.795618Z","shell.execute_reply":"2025-02-07T20:37:49.632024Z"}},"outputs":[{"name":"stdout","text":"Summarized Text: Swansea's Bafetimbi Gomis has a history of fainting .\n","output_type":"stream"}],"execution_count":83},{"cell_type":"code","source":"valid[2]['highlights']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T20:37:52.377371Z","iopub.execute_input":"2025-02-07T20:37:52.377752Z","iopub.status.idle":"2025-02-07T20:37:52.384925Z","shell.execute_reply.started":"2025-02-07T20:37:52.377725Z","shell.execute_reply":"2025-02-07T20:37:52.383955Z"}},"outputs":[{"execution_count":84,"output_type":"execute_result","data":{"text/plain":"'Bafetimbi Gomis collapses within 10 minutes of kickoff at Tottenham .\\nBut he reportedly left the pitch conscious and wearing an oxygen mask .\\nGomis later said that he was \"feeling well\"\\nThe incident came three years after Fabrice Muamba collapsed at White Hart Lane .'"},"metadata":{}}],"execution_count":84},{"cell_type":"markdown","source":"performance evaluation","metadata":{}},{"cell_type":"code","source":"import evaluate\n\ndef calculate_metric_on_test_ds(dataset, metric, model, tokenizer,\n                               batch_size=16, device=None,\n                               column_text=\"article\",\n                               column_summary=\"highlights\"):\n    # Default to GPU if available, otherwise use CPU\n    if device is None:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # Move the model to the correct device (GPU/CPU)\n    model.to(device)\n\n    # Splitting the dataset into batches\n    article_batches = list(generate_batch_sized_chunks(dataset[column_text], batch_size))\n    target_batches = list(generate_batch_sized_chunks(dataset[column_summary], batch_size))\n\n    # Processing each batch\n    for article_batch, target_batch in tqdm(\n        zip(article_batches, target_batches), total=len(article_batches)):\n\n        # Tokenizing the articles:\n        inputs = tokenizer(article_batch, max_length=1024, truncation=True,\n                           padding=\"max_length\", return_tensors=\"pt\")\n\n        # Move inputs to the same device as the model\n        inputs = {key: value.to(device) for key, value in inputs.items()}\n\n        # Generating summaries:\n        summaries = model.generate(input_ids=inputs[\"input_ids\"],\n                                   attention_mask=inputs[\"attention_mask\"],\n                                   length_penalty=0.8, num_beams=8, max_length=128)\n\n        # Decode the generated summaries\n        decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n                             for s in summaries]\n\n        # Replace unwanted tokens (like empty spaces)\n        decoded_summaries = [d.replace(\"\", \" \") for d in decoded_summaries]\n\n        # Updating the metric\n        metric.add_batch(predictions=decoded_summaries, references=target_batch)\n\n    # Compute and return the ROUGE scores.\n    score = metric.compute()\n    return score\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T20:40:35.706516Z","iopub.execute_input":"2025-02-07T20:40:35.706882Z","iopub.status.idle":"2025-02-07T20:40:35.714580Z","shell.execute_reply.started":"2025-02-07T20:40:35.706856Z","shell.execute_reply":"2025-02-07T20:40:35.713427Z"}},"outputs":[],"execution_count":85},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T20:41:04.113689Z","iopub.execute_input":"2025-02-07T20:41:04.114034Z","iopub.status.idle":"2025-02-07T20:41:04.119816Z","shell.execute_reply.started":"2025-02-07T20:41:04.114010Z","shell.execute_reply":"2025-02-07T20:41:04.118840Z"}},"outputs":[{"execution_count":88,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}],"execution_count":88},{"cell_type":"code","source":"y = x['validation'].select(range(0, len(x['validation']), 30))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T20:42:28.756434Z","iopub.execute_input":"2025-02-07T20:42:28.756797Z","iopub.status.idle":"2025-02-07T20:42:28.764899Z","shell.execute_reply.started":"2025-02-07T20:42:28.756771Z","shell.execute_reply":"2025-02-07T20:42:28.764079Z"}},"outputs":[],"execution_count":96},{"cell_type":"code","source":"y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T20:42:30.653082Z","iopub.execute_input":"2025-02-07T20:42:30.653360Z","iopub.status.idle":"2025-02-07T20:42:30.658773Z","shell.execute_reply.started":"2025-02-07T20:42:30.653338Z","shell.execute_reply":"2025-02-07T20:42:30.657942Z"}},"outputs":[{"execution_count":97,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['article', 'highlights', 'id'],\n    num_rows: 446\n})"},"metadata":{}}],"execution_count":97},{"cell_type":"code","source":"rouge_metric = evaluate.load(\"rouge\")\nscore = calculate_metric_on_test_ds(\n    y, rouge_metric, s_model, s_tokenizer, batch_size = 1, column_text = 'article', column_summary= 'highlights'\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T20:42:33.380807Z","iopub.execute_input":"2025-02-07T20:42:33.381124Z","iopub.status.idle":"2025-02-07T20:57:24.025176Z","shell.execute_reply.started":"2025-02-07T20:42:33.381100Z","shell.execute_reply":"2025-02-07T20:57:24.024221Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 446/446 [14:46<00:00,  1.99s/it]\n","output_type":"stream"}],"execution_count":98},{"cell_type":"code","source":"score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T20:57:26.906263Z","iopub.execute_input":"2025-02-07T20:57:26.906632Z","iopub.status.idle":"2025-02-07T20:57:26.912705Z","shell.execute_reply.started":"2025-02-07T20:57:26.906603Z","shell.execute_reply":"2025-02-07T20:57:26.911762Z"}},"outputs":[{"execution_count":99,"output_type":"execute_result","data":{"text/plain":"{'rouge1': 0.02133428011528079,\n 'rouge2': 0.0006971954129873818,\n 'rougeL': 0.02093051883842114,\n 'rougeLsum': 0.02126262873014098}"},"metadata":{}}],"execution_count":99},{"cell_type":"code","source":"valid[0]['article']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T20:31:58.309871Z","iopub.execute_input":"2025-02-07T20:31:58.310248Z","iopub.status.idle":"2025-02-07T20:31:58.317303Z","shell.execute_reply.started":"2025-02-07T20:31:58.310222Z","shell.execute_reply":"2025-02-07T20:31:58.316371Z"}},"outputs":[{"execution_count":76,"output_type":"execute_result","data":{"text/plain":"'(CNN)Share, and your gift will be multiplied. That may sound like an esoteric adage, but when Zully Broussard selflessly decided to give one of her kidneys to a stranger, her generosity paired up with big data. It resulted in six patients receiving transplants. That surprised and wowed her. \"I thought I was going to help this one person who I don\\'t know, but the fact that so many people can have a life extension, that\\'s pretty big,\" Broussard told CNN affiliate KGO. She may feel guided in her generosity by a higher power. \"Thanks for all the support and prayers,\" a comment on a Facebook page in her name read. \"I know this entire journey is much bigger than all of us. I also know I\\'m just the messenger.\" CNN cannot verify the authenticity of the page. But the power that multiplied Broussard\\'s gift was data processing of genetic profiles from donor-recipient pairs. It works on a simple swapping principle but takes it to a much higher level, according to California Pacific Medical Center in San Francisco. So high, that it is taking five surgeons, a covey of physician assistants, nurses and anesthesiologists, and more than 40 support staff to perform surgeries on 12 people. They are extracting six kidneys from donors and implanting them into six recipients. \"The ages of the donors and recipients range from 26 to 70 and include three parent and child pairs, one sibling pair and one brother and sister-in-law pair,\" the medical center said in a statement. The chain of surgeries is to be wrapped up Friday. In late March, the medical center is planning to hold a reception for all 12 patients. Here\\'s how the super swap works, according to California Pacific Medical Center. Say, your brother needs a kidney to save his life, or at least get off of dialysis, and you\\'re willing to give him one of yours. But then it turns out that your kidney is not a match for him, and it\\'s certain his body would reject it. Your brother can then get on a years-long waiting list for a kidney coming from an organ donor who died. Maybe that will work out -- or not, and time could run out for him. Alternatively, you and your brother could look for another recipient-living donor couple like yourselves -- say, two more siblings, where the donor\\'s kidney isn\\'t suited for his sister, the recipient. But maybe your kidney is a match for his sister, and his kidney is a match for your brother. So, you\\'d do a swap. That\\'s called a paired donation. It\\'s a bit of a surgical square dance, where four people cross over partners temporarily and everybody goes home smiling. But instead of a square dance, Broussard\\'s generous move set off a chain reaction, like dominoes falling. Her kidney, which was removed Thursday, went to a recipient, who was paired with a donor. That donor\\'s kidney went to the next recipient, who was also paired with a donor, and so on. On Friday, the last donor will give a kidney to someone who has been biding time on one of those deceased donor lists to complete the chain. Such long-chain transplanting is rare. It\\'s been done before, California Pacific Medical Center said in a statement, but matching up the people in the chain has been laborious and taken a long time. That changed when a computer programmer named David Jacobs received a kidney transplant. He had been waiting on a deceased donor list, when a live donor came along -- someone nice enough to give away a kidney to a stranger. Jacobs paid it forward with his programming skills, creating MatchGrid, a program that genetically matches up donor pairs or chains quickly. \"When we did a five-way swap a few years ago, which was one of the largest, it took about three to four months. We did this in about three weeks,\" Jacobs said. But this chain wouldn\\'t have worked so quickly without Broussard\\'s generosity -- or may not have worked at all. \"The significance of the altruistic donor is that it opens up possibilities for pairing compatible donors and recipients,\" said Dr. Steven Katznelson. \"Where there had been only three or four options, with the inclusion of the altruistic donor, we had 140 options to consider for matching donors and recipients.\" And that\\'s divine, Broussard\\'s friend Shirley Williams wrote in a comment her on Broussard\\'s Facebook page. \"You are a true angel my friend.\"'"},"metadata":{}}],"execution_count":76}]}